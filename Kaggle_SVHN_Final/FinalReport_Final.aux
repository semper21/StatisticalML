\relax 
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  First 10 images in the training set}}{2}}
\newlabel{Figure 1}{{1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Methods and Results}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1}Resources}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2}Data Pre-processing}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3}K-Nearest Neighbor}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4}One vs All}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5}Softmax}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6}Fully Connected Neural Network}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7}Convolutional Neural Network}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  First CNN after implementing additional dropouts}}{6}}
\newlabel{Figure 2}{{2}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Using higher dropout probabilities results in less overfitting. a) Accuracy with dropout p = 0.5. b) Accuracy with dropout p = 0.25}}{7}}
\newlabel{Figure 3}{{3}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Using smaller filter size results in higher accuracy and lower accuracy. a)Accuracy and c)loss of a model with two 3x3 filters. b)Accuracy and d)loss of a model with a 3x3 filter and 5x5 filter.}}{7}}
\newlabel{Figure 4}{{4}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Overfitting is observed faster in FC1024 than in FC512.a)Accuracy and c)loss of a model with FC1024. b)Accuracy and d)loss of a modell with FC512.}}{8}}
\newlabel{Figure 5}{{5}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  As we add more layers, accuracy converges faster. Accuracies with a) three conv layers (32,64,128), b) four conv layers (32,32,64,128), c) five conv layers (32,32,64,64,128) d) six conv layers (32,32,64,64,128,128)}}{8}}
\newlabel{Figure 6}{{6}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Training accuracy with batch normalization converges faster, although validation accuracy in batch normalization does not increase as steadily as in non-batch normalization.}}{9}}
\newlabel{Figure 7}{{7}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  Visualization of weights of the first Conv layer.}}{11}}
\newlabel{Figure 8}{{8}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  Visualization of the first activation layer.}}{11}}
\newlabel{Figure 9}{{9}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  a) The original image of the fourth image in the training set. b) Grayscaled and normalized image of the fourth image in the training set.}}{12}}
\newlabel{Figure 10}{{10}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Discussion}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces  Kaggle submission accuracy over time}}{13}}
\newlabel{Figure 11}{{11}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}References}{14}}
