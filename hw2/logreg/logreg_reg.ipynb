{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting data with green circle indicating (y=1) examples and red circle indicating (y=0) examples ...\n"
     ]
    }
   ],
   "source": [
    "##================ Part 0: Reading data and plotting ==================#\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('ex2data2.txt')\n",
    "X = np.vstack([data.x1,data.x2]).T\n",
    "y = data.y\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plot_utils\n",
    "\n",
    "print 'Plotting data with green circle indicating (y=1) examples and red circle indicating (y=0) examples ...'\n",
    "plot_utils.plot_twoclass_data(X,y,'Chip Test 1', 'Chip Test 2',['y=0','y=1'])\n",
    "#plt.show()\n",
    "plt.savefig('fig3.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.529003\n",
      "         Iterations: 47\n",
      "         Function evaluations: 48\n",
      "         Gradient evaluations: 48\n",
      "Theta found by fmin_bfgs:  [ 1.27268739  0.62557016  1.1809665  -2.01919822 -0.91761468 -1.43194199\n",
      "  0.12375921 -0.36513086 -0.35703388 -0.17485805 -1.45843772 -0.05129676\n",
      " -0.61603963 -0.2746414  -1.19282569 -0.24270336 -0.20570022 -0.04499768\n",
      " -0.27782709 -0.29525851 -0.45613294 -1.04377851  0.02762813 -0.29265642\n",
      "  0.01543393 -0.32759318 -0.14389199 -0.92460119]\n",
      "Final loss =  0.4624583499\n"
     ]
    }
   ],
   "source": [
    "#================ Part 1: Compute cost and gradient ==================#\n",
    "# open logistic_regressor.py and implement the regularized loss function \n",
    "# and gradient \n",
    "\n",
    "# map the features in ex2data2.txt into a pth order polynomial\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Map X onto polynomial features and normalize\n",
    "\n",
    "p = 6\n",
    "poly = sklearn.preprocessing.PolynomialFeatures(degree=p,include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# set up the data matrix (expanded basis functions) with the column of ones as intercept\n",
    "\n",
    "XX = np.vstack([np.ones((X_poly.shape[0],)),X_poly.T]).T\n",
    "\n",
    "# set up a regularized logistic regression model\n",
    "\n",
    "from logistic_regressor import RegLogisticRegressor\n",
    "\n",
    "reg_lr1 = RegLogisticRegressor()\n",
    "\n",
    "# run fmin on the loss function and gradient \n",
    "\n",
    "reg = 1.0\n",
    "theta_opt = reg_lr1.train(XX,y,reg=reg,num_iters=1000,norm=False)\n",
    "\n",
    "# print the theta found and the final loss\n",
    "\n",
    "print 'Theta found by fmin_bfgs: ',theta_opt\n",
    "print \"Final loss = \", reg_lr1.loss(theta_opt,XX,y,0.0)\n",
    "\n",
    "# plot the decision boundary\n",
    "\n",
    "plot_utils.plot_decision_boundary_poly(X,y,theta_opt,reg,p,'Chip Test 1', 'Chip Test 2',['y = 0','y = 1'])\n",
    "#plt.show()\n",
    "plt.savefig('fig4.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training set =  0.830508474576\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy on training set\n",
    "# implement the predict method in logistic_regressor.py\n",
    "\n",
    "reg_lr1.theta = theta_opt\n",
    "predy = reg_lr1.predict(XX)\n",
    "\n",
    "# TODO: fill in the expression for accuracy of prediction\n",
    "accuracy = np.mean(predy == y)\n",
    "print \"Accuracy on the training set = \", accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing learned model with sklearn's logistic ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta found by sklearn with L2 reg:  [ 1.1421394   0.60141117  1.16712554 -1.87160974 -0.91574144 -1.26966693\n",
      "  0.12658629 -0.3686536  -0.34511687 -0.17368655 -1.42387465 -0.04870064\n",
      " -0.60646669 -0.26935562 -1.16303832 -0.24327026 -0.20702143 -0.04326335\n",
      " -0.28028058 -0.286921   -0.46908732 -1.03633961  0.02914775 -0.29263743\n",
      "  0.01728096 -0.32898422 -0.13801971 -0.93196832]\n",
      "Loss with sklearn theta:  0.46843403006\n"
     ]
    }
   ],
   "source": [
    "# Compare with model learned by sklearn's logistic regression with reg = 1/C\n",
    "# the regularization parameter set below can be varied (on a logarithmic scale)\n",
    "\n",
    "reg = 1.0\n",
    "\n",
    "# L2 regularization with sklearn LogisticRegression\n",
    "\n",
    "from sklearn import linear_model\n",
    "sk_logreg_l2 = linear_model.LogisticRegression(C=1.0/reg,solver='lbfgs',fit_intercept=False)\n",
    "sk_logreg_l2.fit(XX,y)\n",
    "print \"Theta found by sklearn with L2 reg: \", sk_logreg_l2.coef_[0]\n",
    "print \"Loss with sklearn theta: \", reg_lr1.loss(sk_logreg_l2.coef_[0],XX,y,0.0)\n",
    "\n",
    "plot_utils.plot_decision_boundary_sklearn_poly(X,y,sk_logreg_l2,reg,p,'Exam 1 score', 'Exam 2 score',['Not Admitted','Admitted'])\n",
    "#plt.show()\n",
    "plt.savefig('fig4_sk.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 regularized logistic regre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta found by sklearn with L1 reg:  [ 1.8700484   0.6867449   1.28051187 -4.8628958  -1.62197231 -2.34477275\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.         -2.3650636   0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.        ]\n",
      "Loss with sklearn theta:  0.438136442481\n",
      "Computing regularization path ...\n"
     ]
    }
   ],
   "source": [
    "# L1 regularization witk sklearn LogisticRegression\n",
    "\n",
    "sk_logreg_l1 = linear_model.LogisticRegression(C=1.0/reg,solver='liblinear',fit_intercept=False,penalty='l1')\n",
    "sk_logreg_l1.fit(XX,y)\n",
    "print \"Theta found by sklearn with L1 reg: \", sk_logreg_l1.coef_[0]\n",
    "print \"Loss with sklearn theta: \", reg_lr1.loss(sk_logreg_l1.coef_[0],XX,y,0.0)\n",
    "\n",
    "# plot regularization paths for L1 regression\n",
    "# Exploration of L1 regularization \n",
    "# \n",
    "plot_utils.plot_regularization_path(XX,y)\n",
    "#plt.show()\n",
    "plt.savefig('fig5.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
