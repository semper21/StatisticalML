{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 10000: loss 0.100000\n",
      "iteration 100 / 10000: loss 0.028190\n",
      "iteration 200 / 10000: loss 0.017769\n",
      "iteration 300 / 10000: loss 0.014181\n",
      "iteration 400 / 10000: loss 0.012271\n",
      "iteration 500 / 10000: loss 0.011083\n",
      "iteration 600 / 10000: loss 0.010240\n",
      "iteration 700 / 10000: loss 0.009578\n",
      "iteration 800 / 10000: loss 0.009056\n",
      "iteration 900 / 10000: loss 0.008641\n",
      "iteration 1000 / 10000: loss 0.008296\n",
      "iteration 1100 / 10000: loss 0.007994\n",
      "iteration 1200 / 10000: loss 0.007751\n",
      "iteration 1300 / 10000: loss 0.007537\n",
      "iteration 1400 / 10000: loss 0.007343\n",
      "iteration 1500 / 10000: loss 0.007172\n",
      "iteration 1600 / 10000: loss 0.007016\n",
      "iteration 1700 / 10000: loss 0.006875\n",
      "iteration 1800 / 10000: loss 0.006746\n",
      "iteration 1900 / 10000: loss 0.006625\n",
      "iteration 2000 / 10000: loss 0.006511\n",
      "iteration 2100 / 10000: loss 0.006402\n",
      "iteration 2200 / 10000: loss 0.006300\n",
      "iteration 2300 / 10000: loss 0.006200\n",
      "iteration 2400 / 10000: loss 0.006104\n",
      "iteration 2500 / 10000: loss 0.006011\n",
      "iteration 2600 / 10000: loss 0.005923\n",
      "iteration 2700 / 10000: loss 0.005839\n",
      "iteration 2800 / 10000: loss 0.005759\n",
      "iteration 2900 / 10000: loss 0.005682\n",
      "iteration 3000 / 10000: loss 0.005608\n",
      "iteration 3100 / 10000: loss 0.005536\n",
      "iteration 3200 / 10000: loss 0.005469\n",
      "iteration 3300 / 10000: loss 0.005405\n",
      "iteration 3400 / 10000: loss 0.005344\n",
      "iteration 3500 / 10000: loss 0.005288\n",
      "iteration 3600 / 10000: loss 0.005231\n",
      "iteration 3700 / 10000: loss 0.005177\n",
      "iteration 3800 / 10000: loss 0.005123\n",
      "iteration 3900 / 10000: loss 0.005071\n",
      "iteration 4000 / 10000: loss 0.005022\n",
      "iteration 4100 / 10000: loss 0.004977\n",
      "iteration 4200 / 10000: loss 0.004930\n",
      "iteration 4300 / 10000: loss 0.004887\n",
      "iteration 4400 / 10000: loss 0.004846\n",
      "iteration 4500 / 10000: loss 0.004805\n",
      "iteration 4600 / 10000: loss 0.004766\n",
      "iteration 4700 / 10000: loss 0.004729\n",
      "iteration 4800 / 10000: loss 0.004692\n",
      "iteration 4900 / 10000: loss 0.004656\n",
      "iteration 5000 / 10000: loss 0.004621\n",
      "iteration 5100 / 10000: loss 0.004586\n",
      "iteration 5200 / 10000: loss 0.004551\n",
      "iteration 5300 / 10000: loss 0.004518\n",
      "iteration 5400 / 10000: loss 0.004486\n",
      "iteration 5500 / 10000: loss 0.004455\n",
      "iteration 5600 / 10000: loss 0.004424\n",
      "iteration 5700 / 10000: loss 0.004394\n",
      "iteration 5800 / 10000: loss 0.004366\n",
      "iteration 5900 / 10000: loss 0.004338\n",
      "iteration 6000 / 10000: loss 0.004308\n",
      "iteration 6100 / 10000: loss 0.004280\n",
      "iteration 6200 / 10000: loss 0.004258\n",
      "iteration 6300 / 10000: loss 0.004227\n",
      "iteration 6400 / 10000: loss 0.004202\n",
      "iteration 6500 / 10000: loss 0.004176\n",
      "iteration 6600 / 10000: loss 0.004152\n",
      "iteration 6700 / 10000: loss 0.004128\n",
      "iteration 6800 / 10000: loss 0.004105\n",
      "iteration 6900 / 10000: loss 0.004084\n",
      "iteration 7000 / 10000: loss 0.004060\n",
      "iteration 7100 / 10000: loss 0.004039\n",
      "iteration 7200 / 10000: loss 0.004018\n",
      "iteration 7300 / 10000: loss 0.003998\n",
      "iteration 7400 / 10000: loss 0.003978\n",
      "iteration 7500 / 10000: loss 0.003958\n",
      "iteration 7600 / 10000: loss 0.003939\n",
      "iteration 7700 / 10000: loss 0.003919\n",
      "iteration 7800 / 10000: loss 0.003901\n",
      "iteration 7900 / 10000: loss 0.003887\n",
      "iteration 8000 / 10000: loss 0.003863\n",
      "iteration 8100 / 10000: loss 0.003846\n",
      "iteration 8200 / 10000: loss 0.003828\n",
      "iteration 8300 / 10000: loss 0.003811\n",
      "iteration 8400 / 10000: loss 0.003794\n",
      "iteration 8500 / 10000: loss 0.003778\n",
      "iteration 8600 / 10000: loss 0.003763\n",
      "iteration 8700 / 10000: loss 0.003750\n",
      "iteration 8800 / 10000: loss 0.003737\n",
      "iteration 8900 / 10000: loss 0.003722\n",
      "iteration 9000 / 10000: loss 0.003707\n",
      "iteration 9100 / 10000: loss 0.003694\n",
      "iteration 9200 / 10000: loss 0.003680\n",
      "iteration 9300 / 10000: loss 0.003668\n",
      "iteration 9400 / 10000: loss 0.003653\n",
      "iteration 9500 / 10000: loss 0.003644\n",
      "iteration 9600 / 10000: loss 0.003632\n",
      "iteration 9700 / 10000: loss 0.003614\n",
      "iteration 9800 / 10000: loss 0.003601\n",
      "iteration 9900 / 10000: loss 0.003588\n",
      "Accuracy of model on training data is:  0.993\n",
      "Accuracy of model on test data is:  0.991\n",
      "Top 15 predictors of spam are: \n",
      "click\n",
      "remov\n",
      "our\n",
      "basenumb\n",
      "guarante\n",
      "pleas\n",
      "you\n",
      "free\n",
      "visit\n",
      "here\n",
      "nbsp\n",
      "will\n",
      "hour\n",
      "most\n",
      "dollar\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing, metrics\n",
    "import utils\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from linear_classifier import LinearSVM_twoclass\n",
    "\n",
    "# load the SPAM email training dataset\n",
    "\n",
    "X,y = utils.load_mat('data/spamTrain.mat')\n",
    "yy = np.ones(y.shape)\n",
    "yy[y==0] = -1\n",
    "\n",
    "# load the SPAM email test dataset\n",
    "\n",
    "test_data = scipy.io.loadmat('data/spamTest.mat')\n",
    "X_test = test_data['Xtest']\n",
    "y_test = test_data['ytest'].flatten()\n",
    "\n",
    "##################################################################################\n",
    "#  YOUR CODE HERE for training the best performing SVM for the data above.       #\n",
    "#  what should C be? What should num_iters be? Should X be scaled?               #\n",
    "#  should X be kernelized? What should the learning rate be? What should the     #\n",
    "#  number of iterations be?                                                      #\n",
    "##################################################################################\n",
    "'''\n",
    "from sklearn import cross_validation\n",
    "XX, XXval, yyy, yyval = cross_validation.train_test_split(X, yy, test_size=0.2)\n",
    "\n",
    "Cvals = [0.1, 0.3, 1, 3, 10, 30]\n",
    "lr_vals = [1e-2, 3e-2, 1e-1, 3e-1, 1, 3]\n",
    "iter_vals = [1000, 5000, 10000, 25000]\n",
    "\n",
    "best_acc = 0\n",
    "scaler = preprocessing.StandardScaler().fit(XX)\n",
    "scaleX = scaler.transform(XX)\n",
    "XX = np.vstack([np.ones((scaleX.shape[0],)), scaleX.T]).T\n",
    "\n",
    "scalerval = preprocessing.StandardScaler().fit(XXval)\n",
    "scaleXval = scalerval.transform(XXval)\n",
    "XXval = np.vstack([np.ones((scaleXval.shape[0],)), scaleXval.T]).T\n",
    "\n",
    "for C in Cvals:\n",
    "    for lr in lr_vals:\n",
    "        for it in iter_vals:\n",
    "            svm = LinearSVM_twoclass()\n",
    "            svm.theta = np.zeros((XX.shape[1],))\n",
    "            svm.train(XX, yyy, learning_rate=lr, reg=C, num_iters=it, verbose=True)\n",
    "            yval_predict = svm.predict(XXval)\n",
    "            acc = metrics.accuracy_score(yyval, yval_predict)\n",
    "            print (\"C {}\".format(C), \"learning rate {}\".format(lr), \"iteration {}\".format(it), \"accuracy {}\".format(acc))\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_C = C\n",
    "                best_lr = lr\n",
    "                best_it = it\n",
    "print ('best', best_C, best_acc, best_lr, best_it)\n",
    "'''\n",
    "svm = LinearSVM_twoclass()\n",
    "svm.theta = np.zeros((X.shape[1],))\n",
    "\n",
    "best_C = 0.1\n",
    "best_lr = 0.1\n",
    "best_it = 10000\n",
    "\n",
    "svm.train(X, yy, learning_rate=1e-1, reg=best_C, num_iters=best_it, verbose=True)\n",
    "##################################################################################\n",
    "# YOUR CODE HERE for testing your best model's perfor                            #\n",
    "# what is the accuracy of your best model on the test set? On the training set?  #\n",
    "##################################################################################\n",
    "\n",
    "y_pred = svm.predict(X)\n",
    "print \"Accuracy of model on training data is: \", metrics.accuracy_score(yy,y_pred)\n",
    "\n",
    "yy_test = np.ones(y_test.shape)\n",
    "yy_test[y_test==0] = -1\n",
    "test_pred = svm.predict(X_test)\n",
    "print \"Accuracy of model on test data is: \", metrics.accuracy_score(yy_test,test_pred)\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "# ANALYSIS OF MODEL: Print the top 15 words that are predictive of spam and for  #\n",
    "# ham. Hint: use the coefficient values of the learned model                     #\n",
    "##################################################################################\n",
    "words, inv_words = utils.get_vocab_dict()\n",
    "\n",
    "index = np.argsort(svm.theta)[-15:]\n",
    "print \"Top 15 predictors of spam are: \"\n",
    "for i in range(-1,-16,-1):\n",
    "    print words[index[i]+1]\n",
    "    \n",
    "##################################################################################\n",
    "#                    END OF YOUR CODE                                            #\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 10000: loss 0.100000\n",
      "iteration 100 / 10000: loss 0.011278\n",
      "iteration 200 / 10000: loss 0.011013\n",
      "iteration 300 / 10000: loss 0.008198\n",
      "iteration 400 / 10000: loss 0.008405\n",
      "iteration 500 / 10000: loss 0.006487\n",
      "iteration 600 / 10000: loss 0.005924\n",
      "iteration 700 / 10000: loss 0.006062\n",
      "iteration 800 / 10000: loss 0.006145\n",
      "iteration 900 / 10000: loss 0.006678\n",
      "iteration 1000 / 10000: loss 0.004963\n",
      "iteration 1100 / 10000: loss 0.006848\n",
      "iteration 1200 / 10000: loss 0.004501\n",
      "iteration 1300 / 10000: loss 0.005626\n",
      "iteration 1400 / 10000: loss 0.006181\n",
      "iteration 1500 / 10000: loss 0.004133\n",
      "iteration 1600 / 10000: loss 0.003925\n",
      "iteration 1700 / 10000: loss 0.003911\n",
      "iteration 1800 / 10000: loss 0.003558\n",
      "iteration 1900 / 10000: loss 0.004517\n",
      "iteration 2000 / 10000: loss 0.004601\n",
      "iteration 2100 / 10000: loss 0.005298\n",
      "iteration 2200 / 10000: loss 0.003560\n",
      "iteration 2300 / 10000: loss 0.003343\n",
      "iteration 2400 / 10000: loss 0.003224\n",
      "iteration 2500 / 10000: loss 0.003342\n",
      "iteration 2600 / 10000: loss 0.003123\n",
      "iteration 2700 / 10000: loss 0.004149\n",
      "iteration 2800 / 10000: loss 0.003084\n",
      "iteration 2900 / 10000: loss 0.002949\n",
      "iteration 3000 / 10000: loss 0.004045\n",
      "iteration 3100 / 10000: loss 0.004987\n",
      "iteration 3200 / 10000: loss 0.003680\n",
      "iteration 3300 / 10000: loss 0.003946\n",
      "iteration 3400 / 10000: loss 0.002664\n",
      "iteration 3500 / 10000: loss 0.004024\n",
      "iteration 3600 / 10000: loss 0.002570\n",
      "iteration 3700 / 10000: loss 0.003536\n",
      "iteration 3800 / 10000: loss 0.002444\n",
      "iteration 3900 / 10000: loss 0.002785\n",
      "iteration 4000 / 10000: loss 0.002592\n",
      "iteration 4100 / 10000: loss 0.002262\n",
      "iteration 4200 / 10000: loss 0.002484\n",
      "iteration 4300 / 10000: loss 0.002317\n",
      "iteration 4400 / 10000: loss 0.002172\n",
      "iteration 4500 / 10000: loss 0.002263\n",
      "iteration 4600 / 10000: loss 0.002059\n",
      "iteration 4700 / 10000: loss 0.002207\n",
      "iteration 4800 / 10000: loss 0.001971\n",
      "iteration 4900 / 10000: loss 0.002106\n",
      "iteration 5000 / 10000: loss 0.001931\n",
      "iteration 5100 / 10000: loss 0.001842\n",
      "iteration 5200 / 10000: loss 0.001914\n",
      "iteration 5300 / 10000: loss 0.002836\n",
      "iteration 5400 / 10000: loss 0.002931\n",
      "iteration 5500 / 10000: loss 0.003760\n",
      "iteration 5600 / 10000: loss 0.001902\n",
      "iteration 5700 / 10000: loss 0.002117\n",
      "iteration 5800 / 10000: loss 0.001842\n",
      "iteration 5900 / 10000: loss 0.003230\n",
      "iteration 6000 / 10000: loss 0.002767\n",
      "iteration 6100 / 10000: loss 0.001846\n",
      "iteration 6200 / 10000: loss 0.003343\n",
      "iteration 6300 / 10000: loss 0.001900\n",
      "iteration 6400 / 10000: loss 0.001530\n",
      "iteration 6500 / 10000: loss 0.001977\n",
      "iteration 6600 / 10000: loss 0.002343\n",
      "iteration 6700 / 10000: loss 0.001604\n",
      "iteration 6800 / 10000: loss 0.001606\n",
      "iteration 6900 / 10000: loss 0.001927\n",
      "iteration 7000 / 10000: loss 0.003566\n",
      "iteration 7100 / 10000: loss 0.003050\n",
      "iteration 7200 / 10000: loss 0.001414\n",
      "iteration 7300 / 10000: loss 0.003359\n",
      "iteration 7400 / 10000: loss 0.001830\n",
      "iteration 7500 / 10000: loss 0.001382\n",
      "iteration 7600 / 10000: loss 0.001819\n",
      "iteration 7700 / 10000: loss 0.001371\n",
      "iteration 7800 / 10000: loss 0.001291\n",
      "iteration 7900 / 10000: loss 0.001297\n",
      "iteration 8000 / 10000: loss 0.001259\n",
      "iteration 8100 / 10000: loss 0.001313\n",
      "iteration 8200 / 10000: loss 0.002427\n",
      "iteration 8300 / 10000: loss 0.001328\n",
      "iteration 8400 / 10000: loss 0.002475\n",
      "iteration 8500 / 10000: loss 0.001395\n",
      "iteration 8600 / 10000: loss 0.001302\n",
      "iteration 8700 / 10000: loss 0.001557\n",
      "iteration 8800 / 10000: loss 0.001166\n",
      "iteration 8900 / 10000: loss 0.001176\n",
      "iteration 9000 / 10000: loss 0.001575\n",
      "iteration 9100 / 10000: loss 0.001119\n",
      "iteration 9200 / 10000: loss 0.001117\n",
      "iteration 9300 / 10000: loss 0.001129\n",
      "iteration 9400 / 10000: loss 0.001117\n",
      "iteration 9500 / 10000: loss 0.001169\n",
      "iteration 9600 / 10000: loss 0.001070\n",
      "iteration 9700 / 10000: loss 0.001146\n",
      "iteration 9800 / 10000: loss 0.001351\n",
      "iteration 9900 / 10000: loss 0.001097\n",
      "Accuracy of model on training data is:  0.9985\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1000,1000) and (4000,) not aligned: 1000 (dim 1) != 4000 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-59684c7d0666>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0myy_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0myy_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaleKtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Accuracy of model on test data is: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myy_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/minhpham/Desktop/Statistical_Machine_Learning/hw4/linear_classifier.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1000,1000) and (4000,) not aligned: 1000 (dim 1) != 4000 (dim 0)"
     ]
    }
   ],
   "source": [
    "#Cosine similarity kernels\n",
    "\n",
    "XK = sklearn.metrics.pairwise.cosine_similarity(X, Y=None, dense_output=True)\n",
    "scaler = preprocessing.StandardScaler().fit(XK)\n",
    "scaleK = scaler.transform(XK)\n",
    "KK = np.vstack([np.ones((scaleK.shape[0],)),scaleK]).T\n",
    "\n",
    "XKtest = sklearn.metrics.pairwise.cosine_similarity(X_test, Y=None, dense_output=True)\n",
    "scalertest = preprocessing.StandardScaler().fit(XKtest)\n",
    "scaleKtest = scalertest.transform(XKtest)\n",
    "KKtest = np.vstack([np.ones((scaleKtest.shape[0],)),scaleKtest]).T\n",
    "\n",
    "svm = LinearSVM_twoclass()\n",
    "svm.theta = np.zeros((XK.shape[1],))\n",
    "\n",
    "svm.train(scaleK, yy, learning_rate=best_lr, reg=best_C, num_iters=best_it, verbose=True)\n",
    "\n",
    "y_pred = svm.predict(scaleK)\n",
    "print \"Accuracy of model on training data is: \", metrics.accuracy_score(yy,y_pred)\n",
    "\n",
    "yy_test = np.ones(y_test.shape)\n",
    "yy_test[y_test==0] = -1\n",
    "test_pred = svm.predict(scaleKtest)\n",
    "print \"Accuracy of model on test data is: \", metrics.accuracy_score(yy_test,test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Gaussian Kernels\n",
    "\n",
    "sigma = 0.02\n",
    "\n",
    "K = np.array([utils.gaussian_kernel(x1,x2,sigma) for x1 in X for x2 in X]).reshape(X.shape[0],X.shape[0])\n",
    "scaler = preprocessing.StandardScaler().fit(K)\n",
    "scaleK = scaler.transform(K)\n",
    "KK = np.vstack([np.ones((scaleK.shape[0],)),scaleK]).T\n",
    "yy = np.ones(y.shape)\n",
    "yy[y == 0] = -1\n",
    "svm = LinearSVM_twoclass()\n",
    "svm.theta = np.zeros((KK.shape[1],))\n",
    "C = 1\n",
    "svm.train(KK,yy,learning_rate=best_lr,reg=best_C,num_iters=best_it,verbose=True)\n",
    "y_pred = svm.predict(KK)\n",
    "print \"Accuracy on training data = \", metrics.accuracy_score(yy,y_pred)\n",
    "yy_test = np.ones(y_test.shape)\n",
    "yy_test[y_test==0] = -1\n",
    "test_pred = svm.predict(X_test)\n",
    "print \"Accuracy of model on test data is: \", metrics.accuracy_score(yy_test,test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
