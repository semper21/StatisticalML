{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 1 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]] [1 1 0 ..., 1 0 0]\n",
      "(4000, 1899) (4000,)\n",
      "iteration 0 / 10000: loss 0.100000\n",
      "iteration 100 / 10000: loss 0.028190\n",
      "iteration 200 / 10000: loss 0.017769\n",
      "iteration 300 / 10000: loss 0.014181\n",
      "iteration 400 / 10000: loss 0.012271\n",
      "iteration 500 / 10000: loss 0.011083\n",
      "iteration 600 / 10000: loss 0.010240\n",
      "iteration 700 / 10000: loss 0.009578\n",
      "iteration 800 / 10000: loss 0.009056\n",
      "iteration 900 / 10000: loss 0.008641\n",
      "iteration 1000 / 10000: loss 0.008296\n",
      "iteration 1100 / 10000: loss 0.007994\n",
      "iteration 1200 / 10000: loss 0.007751\n",
      "iteration 1300 / 10000: loss 0.007537\n",
      "iteration 1400 / 10000: loss 0.007343\n",
      "iteration 1500 / 10000: loss 0.007172\n",
      "iteration 1600 / 10000: loss 0.007016\n",
      "iteration 1700 / 10000: loss 0.006875\n",
      "iteration 1800 / 10000: loss 0.006746\n",
      "iteration 1900 / 10000: loss 0.006625\n",
      "iteration 2000 / 10000: loss 0.006511\n",
      "iteration 2100 / 10000: loss 0.006402\n",
      "iteration 2200 / 10000: loss 0.006300\n",
      "iteration 2300 / 10000: loss 0.006200\n",
      "iteration 2400 / 10000: loss 0.006104\n",
      "iteration 2500 / 10000: loss 0.006011\n",
      "iteration 2600 / 10000: loss 0.005923\n",
      "iteration 2700 / 10000: loss 0.005839\n",
      "iteration 2800 / 10000: loss 0.005759\n",
      "iteration 2900 / 10000: loss 0.005682\n",
      "iteration 3000 / 10000: loss 0.005608\n",
      "iteration 3100 / 10000: loss 0.005536\n",
      "iteration 3200 / 10000: loss 0.005469\n",
      "iteration 3300 / 10000: loss 0.005405\n",
      "iteration 3400 / 10000: loss 0.005344\n",
      "iteration 3500 / 10000: loss 0.005288\n",
      "iteration 3600 / 10000: loss 0.005231\n",
      "iteration 3700 / 10000: loss 0.005177\n",
      "iteration 3800 / 10000: loss 0.005123\n",
      "iteration 3900 / 10000: loss 0.005071\n",
      "iteration 4000 / 10000: loss 0.005022\n",
      "iteration 4100 / 10000: loss 0.004977\n",
      "iteration 4200 / 10000: loss 0.004930\n",
      "iteration 4300 / 10000: loss 0.004887\n",
      "iteration 4400 / 10000: loss 0.004846\n",
      "iteration 4500 / 10000: loss 0.004805\n",
      "iteration 4600 / 10000: loss 0.004766\n",
      "iteration 4700 / 10000: loss 0.004729\n",
      "iteration 4800 / 10000: loss 0.004692\n",
      "iteration 4900 / 10000: loss 0.004656\n",
      "iteration 5000 / 10000: loss 0.004621\n",
      "iteration 5100 / 10000: loss 0.004586\n",
      "iteration 5200 / 10000: loss 0.004551\n",
      "iteration 5300 / 10000: loss 0.004518\n",
      "iteration 5400 / 10000: loss 0.004486\n",
      "iteration 5500 / 10000: loss 0.004455\n",
      "iteration 5600 / 10000: loss 0.004424\n",
      "iteration 5700 / 10000: loss 0.004394\n",
      "iteration 5800 / 10000: loss 0.004366\n",
      "iteration 5900 / 10000: loss 0.004338\n",
      "iteration 6000 / 10000: loss 0.004308\n",
      "iteration 6100 / 10000: loss 0.004280\n",
      "iteration 6200 / 10000: loss 0.004258\n",
      "iteration 6300 / 10000: loss 0.004227\n",
      "iteration 6400 / 10000: loss 0.004202\n",
      "iteration 6500 / 10000: loss 0.004176\n",
      "iteration 6600 / 10000: loss 0.004152\n",
      "iteration 6700 / 10000: loss 0.004128\n",
      "iteration 6800 / 10000: loss 0.004105\n",
      "iteration 6900 / 10000: loss 0.004084\n",
      "iteration 7000 / 10000: loss 0.004060\n",
      "iteration 7100 / 10000: loss 0.004039\n",
      "iteration 7200 / 10000: loss 0.004018\n",
      "iteration 7300 / 10000: loss 0.003998\n",
      "iteration 7400 / 10000: loss 0.003978\n",
      "iteration 7500 / 10000: loss 0.003958\n",
      "iteration 7600 / 10000: loss 0.003939\n",
      "iteration 7700 / 10000: loss 0.003919\n",
      "iteration 7800 / 10000: loss 0.003901\n",
      "iteration 7900 / 10000: loss 0.003887\n",
      "iteration 8000 / 10000: loss 0.003863\n",
      "iteration 8100 / 10000: loss 0.003846\n",
      "iteration 8200 / 10000: loss 0.003828\n",
      "iteration 8300 / 10000: loss 0.003811\n",
      "iteration 8400 / 10000: loss 0.003794\n",
      "iteration 8500 / 10000: loss 0.003778\n",
      "iteration 8600 / 10000: loss 0.003763\n",
      "iteration 8700 / 10000: loss 0.003750\n",
      "iteration 8800 / 10000: loss 0.003737\n",
      "iteration 8900 / 10000: loss 0.003722\n",
      "iteration 9000 / 10000: loss 0.003707\n",
      "iteration 9100 / 10000: loss 0.003694\n",
      "iteration 9200 / 10000: loss 0.003680\n",
      "iteration 9300 / 10000: loss 0.003668\n",
      "iteration 9400 / 10000: loss 0.003653\n",
      "iteration 9500 / 10000: loss 0.003644\n",
      "iteration 9600 / 10000: loss 0.003632\n",
      "iteration 9700 / 10000: loss 0.003614\n",
      "iteration 9800 / 10000: loss 0.003601\n",
      "iteration 9900 / 10000: loss 0.003588\n",
      "Accuracy of model on training data is:  0.993\n",
      "Accuracy of model on test data is:  0.991\n",
      "Top 15 predictors of spam are: \n",
      "click\n",
      "remov\n",
      "our\n",
      "basenumb\n",
      "guarante\n",
      "pleas\n",
      "you\n",
      "free\n",
      "visit\n",
      "here\n",
      "nbsp\n",
      "will\n",
      "hour\n",
      "most\n",
      "dollar\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing, metrics\n",
    "import utils\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from linear_classifier import LinearSVM_twoclass\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# load the SPAM email training dataset\n",
    "\n",
    "X_train,y_train = utils.load_mat('data/spamTrain.mat')\n",
    "print X_train, y_train\n",
    "print X_train.shape, y_train.shape\n",
    "yy_train = np.ones(y_train.shape)\n",
    "yy_train[y_train==0] = -1\n",
    "\n",
    "# load the SPAM email test dataset\n",
    "\n",
    "test_data = scipy.io.loadmat('data/spamTest.mat')\n",
    "X_test = test_data['Xtest']\n",
    "y_test = test_data['ytest'].flatten()\n",
    "\n",
    "##################################################################################\n",
    "#  YOUR CODE HERE for training the best performing SVM for the data above.       #\n",
    "#  what should C be? What should num_iters be? Should X be scaled?               #\n",
    "#  should X be kernelized? What should the learning rate be? What should the     #\n",
    "#  number of iterations be?                                                      #\n",
    "##################################################################################\n",
    "\n",
    "#svm = LinearSVM_twoclass()\n",
    "#svm.theta = np.zeros((X.shape[1],))\n",
    "\n",
    "from sklearn import cross_validation\n",
    "XX, XXval, yy, yyval = cross_validation.train_test_split(X_train, yy_train, test_size=0.2)\n",
    "\n",
    "C_vals = [0.1,0.3,1,3,10,30]\n",
    "lr_vals = [1e-2, 3e-2, 1e-1, 3e-1, 1, 3]\n",
    "\n",
    "# compute the kernel\n",
    "K = np.array([cosine_similiarity(x) for x in XX]).reshape(XX.shape[0],XX.shape[0])\n",
    "# scale the kernelized data matrix\n",
    "scaler = preprocessing.StandardScaler().fit(K)\n",
    "scaleK = scaler.transform(K)\n",
    "# add the intercept term\n",
    "KK = np.vstack([np.ones((scaleK.shape[0],)),scaleK.T]).T\n",
    "\n",
    "# for validation set\n",
    "Kval = np.array([cosine_similiarity(x) for x in XXval]).reshape(XXval.shape[0],XXval.shape[0])\n",
    "scaler = preprocessing.StandardScaler().fit(Kval)\n",
    "scaleKval = scaler.transform(Kval)\n",
    "KKval = np.vstack([np.ones((scaleKval.shape[0],)),scaleKval.T]).T\n",
    "\n",
    "for lr in lr_vals:\n",
    "    for c in C_vals:\n",
    "        svm.theta = np.zeros((KK.shape[1],))\n",
    "        #train on X, y\n",
    "        svm.train(KK,yy,learning_rate=lr,reg=c,num_iters=10000,verbose=False)\n",
    "        #predict on Xval, yval\n",
    "        y_pred = svm.predict(KKval)\n",
    "        accuracy = metrics.accuracy_score(yyval, y_pred)\n",
    "        print 'lr=', lr, 'c=', c, 'accuracy=', accuracy\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "# YOUR CODE HERE for testing your best model's perfor                            #\n",
    "# what is the accuracy of your best model on the test set? On the training set?  #\n",
    "##################################################################################\n",
    "\n",
    "best_C = 0.1\n",
    "best_lr = 1e-1\n",
    "\n",
    "#best_svm\n",
    "best_svm = LinearSVM()\n",
    "\n",
    "\n",
    "# compute the kernel\n",
    "K = np.array([cosine_similiarity(x) for x in X_test]).reshape(X_test.shape[0],X_test.shape[0])\n",
    "# scale the kernelized data matrix\n",
    "scaler = preprocessing.StandardScaler().fit(K)\n",
    "scaleK = scaler.transform(K)\n",
    "# add the intercept term\n",
    "KK = np.vstack([np.ones((scaleK.shape[0],)),scaleK.T]).T\n",
    "\n",
    "best_svm.theta = np.zeros((KK.shape[1],))\n",
    "best_svm.train(KK,y_test,learning_rate=best_lr,reg=best_C,num_iters=10000,verbose=False)\n",
    "\n",
    "\n",
    "best_svm.train(X_train, yy_train, learning_rate=best_param['LR'], reg=best_param['reg'],\n",
    "                      num_iters=1500, verbose=True)\n",
    "# Evaluate the best svm on test set\n",
    "y_test_pred = best_svm.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print 'linear SVM on raw pixels final test set accuracy: %f' % test_accuracy\n",
    "\n",
    "\n",
    "\n",
    "y_pred = svm.predict(X)\n",
    "print \"Accuracy of model on training data is: \", metrics.accuracy_score(yy,y_pred)\n",
    "\n",
    "yy_test = np.ones(y_test.shape)\n",
    "yy_test[y_test==0] = -1\n",
    "test_pred = svm.predict(X_test)\n",
    "print \"Accuracy of model on test data is: \", metrics.accuracy_score(yy_test,test_pred)\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "# ANALYSIS OF MODEL: Print the top 15 words that are predictive of spam and for  #\n",
    "# ham. Hint: use the coefficient values of the learned model                     #\n",
    "##################################################################################\n",
    "words, inv_words = utils.get_vocab_dict()\n",
    "\n",
    "index = np.argsort(svm.theta)[-15:]\n",
    "print \"Top 15 predictors of spam are: \"\n",
    "for i in range(-1,-16,-1):\n",
    "    print words[index[i]+1]\n",
    "##################################################################################\n",
    "#                    END OF YOUR CODE                                            #\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
