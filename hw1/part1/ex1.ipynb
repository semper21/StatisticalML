{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                                                                                       \n",
    "### Homework 1: Part A: Linear Regression with the Boston Housing data: Single variable linear regression   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "This file contains code that helps you get started on linear regression. You will need to complete the functions \n",
    "in linear_regressor.py and utils.py in the places indicated. Modify these files at the places marked TODO. \n",
    "\n",
    "You will implement linear regression with one variable to predict the median value of a home in\n",
    "a census tract in the Boston suburbs from the percentage of the population in the census tract\n",
    "that is of lower economic status. The file housing.data.txt contains the data for our linear\n",
    "regression problem. We will build a model that predicts the median home value in a census tract\n",
    "(in $10000s) from the percentage of the population of lower economic status in a tract. This notebook\n",
    "has already been set up to load this data for you using the Python package pandas. For\n",
    "a quick introduction to this package, read the python_for_ml.pdf slide deck in the Links to background material Page under the Pages sidebar tab on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data and plotting\n",
    "Before starting on any task, it is often useful to understand the data by visualizing it. For this\n",
    "dataset, you can use a scatter plot to visualize the data, since it has only two features to plot\n",
    "(percentage of population of lower economic status and median home value). Many other problems\n",
    "that you will encounter in real life are multi-dimensional and cannot be plotted on a 2-d plot. We have loaded the predictor and predicted variables in X and y. You will see the plot in\n",
    "Figure 1 generated by plot_utils.py saved in fig1.pdf in the part1 folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data ...\n",
      "Plotting data ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print 'Reading data ...'\n",
    "bdata = load_boston()\n",
    "df = pd.DataFrame(data = bdata.data, columns = bdata.feature_names)\n",
    "\n",
    "#  X is the percentage of the population in a census tract that is of\n",
    "#  lower economic status. X is a vector of length 506.\n",
    "#  y is to the median home value in $10000's. y is a vector of length 506\n",
    "\n",
    "X = df.LSTAT\n",
    "y = bdata.target\n",
    "\n",
    "# Scatter plot LSTAT vs median home value, shown interactively or saved in fig1.pdf\n",
    "\n",
    "import numpy as np\n",
    "import plot_utils\n",
    "print 'Plotting data ...'\n",
    "plot_utils.plot_data(X,y,'LSTAT(x)','MEDV(y)')\n",
    "#plt.show()\n",
    "plt.savefig('fig1.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a univariate model\n",
    "Before you run the contents of the cell below, you will need to implement two functions: one for calcuating the loss $J(\\theta)$, and the other to perform gradient descent in the $\\theta$ space.\n",
    "Once you complete these functions, you can\n",
    "check the convergence of your gradient descent implementation. \n",
    "#### First, complete the code for the loss method for the LinearReg_SquaredLoss class in the file linear_regressor.py. \n",
    "Remember that the variables X and y are not scalar values, but matrices whose rows represent the\n",
    "examples from the training set. \n",
    "#### Next, implement gradient descent in the method train for the LinearRegressor class in the file linear_regressor.py. \n",
    "The loop structure has been written for you, and you only need\n",
    "to supply the updates to $\\theta$ within each iteration. Recall that we minimize the value of $J(\\theta)$ by\n",
    "changing the values of the vector $\\theta$, not by changing X or y. A good way to verify that gradient\n",
    "descent is working correctly is to look at the value of $J(\\theta)$ and check that it is decreasing with each\n",
    "step. The train method calls the loss method on every iteration and prints the cost. Assuming\n",
    "you have implemented gradient descent and and the loss function correctly, your value of $J(\\theta)$\n",
    "should never increase, and should converge to a steady value by the end of the algorithm. You\n",
    "should expect to see a cost of approximately 296.07 at the first iteration. After you are finished,\n",
    "the script below will use your final parameters to plot the linear fit. The result should look something like\n",
    "Figure 2 in the assignment handout. The plot of the $J(\\theta)$ values during gradient descent should resemble the plot in Figure 3 in the assignment handout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 10000: loss 296.073458\n",
      "iteration 1000 / 10000: loss 32.190429\n",
      "iteration 2000 / 10000: loss 20.410446\n",
      "iteration 3000 / 10000: loss 19.347011\n",
      "iteration 4000 / 10000: loss 19.251010\n",
      "iteration 5000 / 10000: loss 19.242344\n",
      "iteration 6000 / 10000: loss 19.241561\n",
      "iteration 7000 / 10000: loss 19.241491\n",
      "iteration 8000 / 10000: loss 19.241484\n",
      "iteration 9000 / 10000: loss 19.241484\n",
      "Theta found by gradient_descent:  [ 34.55363411  -0.95003694]\n"
     ]
    }
   ],
   "source": [
    "# Predict median home value from percentage of lower economic status in a census tract\n",
    "\n",
    "# add the column of ones to X to represent the intercept term\n",
    "\n",
    "XX = np.vstack([np.ones((X.shape[0],)),X]).T\n",
    "\n",
    "from linear_regressor import LinearRegressor, LinearReg_SquaredLoss\n",
    "\n",
    "# set up a linear regression model\n",
    "\n",
    "linear_reg = LinearReg_SquaredLoss()\n",
    "\n",
    "# run gradient descent\n",
    "\n",
    "J_history = linear_reg.train(XX,y,learning_rate=0.005,num_iters=10000,verbose=True)\n",
    "\n",
    "# print the theta found\n",
    "\n",
    "print 'Theta found by gradient_descent: ',linear_reg.theta\n",
    "\n",
    "# plot the linear fit and save it in fig2.pdf\n",
    "plot_utils.plot_data(X,y,'LSTAT(x)','MEDV(y)')\n",
    "plt.plot(X, np.dot(XX,linear_reg.theta), 'g-',linewidth=3)\n",
    "#plt.show()\n",
    "plt.savefig('fig2.pdf')\n",
    "\n",
    "# Plot the convergence graph and save it in fig4.pdf\n",
    "\n",
    "plot_utils.plot_data(range(len(J_history)),J_history,'Number of iterations','Cost J')\n",
    "#plt.show()\n",
    "plt.savefig('fig4.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative analysis of the linear fit\n",
    "What can you say about the quality of the linear fit for this data? In your assignment writeup, explain how you expect the model to perform at the low and high ends of values for LSTAT? How could we improve the quality of the fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on unseen data with the model\n",
    "Your final values for $\\theta$ will also be used to make predictions on median home values for census\n",
    "tracts where the percentage of the population of lower economic status is 5% and 50%. \n",
    "#### First,complete the predict method in the LinearRegressor class in linear regression.py. \n",
    "Then fill\n",
    "in code for prediction using the computed $\\theta$ at the indicated point in the box below. Report\n",
    "the predictions of your model in your lab writeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For lower status percentage = 5, we predict a median home value of 29.8034494122\n",
      "For lower status percentage = 50, we predict a median home value of -12.9482128898\n"
     ]
    }
   ],
   "source": [
    "# Predict values for lower status percentage of 5% and 50%\n",
    "# remember to multiply prediction by 10000 because median value is in 10000s\n",
    "\n",
    "###########################################################################\n",
    "#   TODO:                                                                 #\n",
    "#   Predicted median value of a home with LSTAT = 5%                      #\n",
    "#   Hint: call the predict method with the appropriate x                  #\n",
    "#         One line of code expected; replace line pred_cost = 0           #\n",
    "###########################################################################\n",
    "\n",
    "pred_cost = linear_reg.predict(np.array([1, 5])) \n",
    "print 'For lower status percentage = 5, we predict a median home value of', pred_cost\n",
    "\n",
    "###########################################################################\n",
    "#   TODO:                                                                 #\n",
    "#   Predicted median value of a home with LSTAT = 50%                     #\n",
    "#      One line of code expected, replace pred_cost = 0                   #\n",
    "###########################################################################\n",
    "\n",
    "pred_cost = linear_reg.predict(np.array([1, 50]))\n",
    "print 'For lower status percentage = 50, we predict a median home value of',pred_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing $J(\\theta_0,\\theta_1)$\n",
    "To understand the cost function $J(\\theta_0,\\theta_1)$ better, we plot the cost over a 2-dimensional grid of $\\theta_0$ and $\\theta_1$\n",
    "values. You will not need to code anything new for this part, but you should understand how the\n",
    "code you have written already is creating these images. In the script below, we calculate $J(\\theta_0,\\theta_1)$ over a grid of\n",
    "$(\\theta_0,\\theta_1)$ values using the loss method that you wrote. The 2-D array of $J(\\theta_0,\\theta_1)$ values is plotted using\n",
    "the surf and contour commands of matplotlib. The plots should look something like Figure 4 in the assignment handout.\n",
    "The purpose of these plots is to show you how $J(\\theta_0,\\theta_1)$ varies with changes in $\\theta_0$ and $\\theta_1$. The cost\n",
    "function is bowl-shaped and has a global minimum. This is easier to see in the contour plot than\n",
    "in the 3D surface plot. This minimum is the optimal point for $\\theta_0$ and $\\theta_1$, and each step of gradient\n",
    "descent moves closer to this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing J(theta_0, theta_1) ...\n"
     ]
    }
   ],
   "source": [
    "print 'Visualizing J(theta_0, theta_1) ...'\n",
    "\n",
    "# Compute grid over which we will calculate J\n",
    "\n",
    "theta0_vals = np.arange(-20,40, 0.1);\n",
    "theta1_vals = np.arange(-4, 4, 0.1);\n",
    "J_vals = np.zeros((len(theta0_vals),len(theta1_vals)))\n",
    "\n",
    "# Fill out J_vals and save plots in fig3a.pdf and fig3b.pdf\n",
    "\n",
    "linear_reg2 = LinearReg_SquaredLoss()\n",
    "\n",
    "for i in range(len(theta0_vals)):\n",
    "    for j in range(len(theta1_vals)):\n",
    "        linear_reg2.theta = (theta0_vals[i],theta1_vals[j])\n",
    "        J_vals[i,j],_ = linear_reg2.loss(XX,y)\n",
    "          \n",
    "# Surface and contour plots\n",
    "\n",
    "# Need to transpose J_vals before calling plot functions\n",
    "\n",
    "J_vals = J_vals.T\n",
    "tt1,tt2 = np.meshgrid(theta0_vals,theta1_vals)\n",
    "plot_utils.make_surface_plot(tt1,tt2,J_vals,'$Theta_0$','$Theta_1$')\n",
    "plt.savefig('fig3a.pdf')\n",
    "plot_utils.make_contour_plot(tt1,tt2,J_vals,np.logspace(-10,40,200),'$Theta_0$','$Theta_1$',linear_reg.theta)\n",
    "#plt.show()\n",
    "plt.savefig('fig3b.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing with sklearn's linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients computed by sklearn:  34.5538408794  and  -0.950049353758\n"
     ]
    }
   ],
   "source": [
    "# Check if the model you learned using gradient descent matches the one\n",
    "# that sklearn's linear regression model learns on the same data.\n",
    "\n",
    "from sklearn import linear_model\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(XX,y)\n",
    "\n",
    "print \"The coefficients computed by sklearn: \", lr.intercept_, \" and \", lr.coef_[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Assessing model quality\n",
    "Up to now, we have used all the data to estimate the model. Then, we check  its performance on two unseen points. To assess model quality in a more systematic fashion, we will explore train/test splits and crossvalidation approaches. We will use the mean squared error and the $R^2$ values as measures of model quality. The lower the mean squared error, the better the model is. The closer $R^2$ is to 1., the better the model is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Estimating model on training data -------------\n",
      "iteration 0 / 10000: loss 296.073458\n",
      "iteration 1000 / 10000: loss 32.190429\n",
      "iteration 2000 / 10000: loss 20.410446\n",
      "iteration 3000 / 10000: loss 19.347011\n",
      "iteration 4000 / 10000: loss 19.251010\n",
      "iteration 5000 / 10000: loss 19.242344\n",
      "iteration 6000 / 10000: loss 19.241561\n",
      "iteration 7000 / 10000: loss 19.241491\n",
      "iteration 8000 / 10000: loss 19.241484\n",
      "iteration 9000 / 10000: loss 19.241484\n",
      "Theta found by gradient_descent on training data:  [ 34.55363411  -0.95003694]\n",
      "------------- Evaluating model on test data -------------\n",
      "Residual mean squared error:  35.4024289621\n",
      "Variance explained by model:  0.56712645939\n",
      "------------- Estimating/Evaluating model by crossvalidation -------------\n",
      "iteration 0 / 10000: loss 303.118502\n",
      "iteration 1000 / 10000: loss 35.238515\n",
      "iteration 2000 / 10000: loss 22.628279\n",
      "iteration 3000 / 10000: loss 21.479423\n",
      "iteration 4000 / 10000: loss 21.374756\n",
      "iteration 5000 / 10000: loss 21.365220\n",
      "iteration 6000 / 10000: loss 21.364352\n",
      "iteration 7000 / 10000: loss 21.364272\n",
      "iteration 8000 / 10000: loss 21.364265\n",
      "iteration 9000 / 10000: loss 21.364265\n",
      "Residual mean squared error:  23.557634795\n",
      "Variance explained by model:  0.31786588392\n",
      "iteration 0 / 10000: loss 284.188716\n",
      "iteration 1000 / 10000: loss 31.234345\n",
      "iteration 2000 / 10000: loss 20.076450\n",
      "iteration 3000 / 10000: loss 19.082816\n",
      "iteration 4000 / 10000: loss 18.994331\n",
      "iteration 5000 / 10000: loss 18.986451\n",
      "iteration 6000 / 10000: loss 18.985750\n",
      "iteration 7000 / 10000: loss 18.985687\n",
      "iteration 8000 / 10000: loss 18.985682\n",
      "iteration 9000 / 10000: loss 18.985681\n",
      "Residual mean squared error:  41.821945299\n",
      "Variance explained by model:  0.540603725401\n",
      "iteration 0 / 10000: loss 249.897210\n",
      "iteration 1000 / 10000: loss 28.596381\n",
      "iteration 2000 / 10000: loss 17.098133\n",
      "iteration 3000 / 10000: loss 15.796102\n",
      "iteration 4000 / 10000: loss 15.648664\n",
      "iteration 5000 / 10000: loss 15.631968\n",
      "iteration 6000 / 10000: loss 15.630078\n",
      "iteration 7000 / 10000: loss 15.629864\n",
      "iteration 8000 / 10000: loss 15.629840\n",
      "iteration 9000 / 10000: loss 15.629837\n",
      "Residual mean squared error:  73.9968055907\n",
      "Variance explained by model:  0.0760470759396\n",
      "iteration 0 / 10000: loss 308.907778\n",
      "iteration 1000 / 10000: loss 32.281782\n",
      "iteration 2000 / 10000: loss 19.316662\n",
      "iteration 3000 / 10000: loss 18.033820\n",
      "iteration 4000 / 10000: loss 17.906888\n",
      "iteration 5000 / 10000: loss 17.894329\n",
      "iteration 6000 / 10000: loss 17.893086\n",
      "iteration 7000 / 10000: loss 17.892964\n",
      "iteration 8000 / 10000: loss 17.892951\n",
      "iteration 9000 / 10000: loss 17.892950\n",
      "Residual mean squared error:  50.5004502309\n",
      "Variance explained by model:  0.424245991773\n",
      "iteration 0 / 10000: loss 334.272481\n",
      "iteration 1000 / 10000: loss 32.784978\n",
      "iteration 2000 / 10000: loss 22.063887\n",
      "iteration 3000 / 10000: loss 21.304138\n",
      "iteration 4000 / 10000: loss 21.250299\n",
      "iteration 5000 / 10000: loss 21.246484\n",
      "iteration 6000 / 10000: loss 21.246213\n",
      "iteration 7000 / 10000: loss 21.246194\n",
      "iteration 8000 / 10000: loss 21.246193\n",
      "iteration 9000 / 10000: loss 21.246193\n",
      "Residual mean squared error:  23.2176807345\n",
      "Variance explained by model:  0.126771322851\n",
      "5  fold cross_validation MSE =  42.61890333\n",
      "5  fold cross_validation r_squared =  0.297106799977\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, metrics\n",
    "\n",
    "# split X into training and test sets (80/20 split)\n",
    "\n",
    "X_train,X_test,y_train,y_test = model_selection.train_test_split(XX,y,test_size=0.2)\n",
    "\n",
    "## =======================  Estimating model on training data ===========\n",
    "\n",
    "print '------------- Estimating model on training data -------------'\n",
    "\n",
    "# set up linear regression model and estimate parameters on training data\n",
    "\n",
    "lr = LinearReg_SquaredLoss()\n",
    "J_history = lr.train(XX,y,learning_rate=0.005,num_iters=10000,verbose=True)\n",
    "print 'Theta found by gradient_descent on training data: ',lr.theta\n",
    "\n",
    "## ======================= Evaluating model on test data ===============\n",
    "\n",
    "print '------------- Evaluating model on test data -------------'\n",
    "\n",
    "def evaluate_model(lr,X_test,y_test):\n",
    "    y_pred = lr.predict(X_test)\n",
    "    mse = np.mean((y_pred - y_test) **2)\n",
    "    print \"Residual mean squared error: \", mse\n",
    "    r_squared = metrics.r2_score(y_test,y_pred)\n",
    "    print \"Variance explained by model: \", r_squared\n",
    "    return mse, r_squared\n",
    "\n",
    "mse, r_squared = evaluate_model(lr,X_test,y_test)\n",
    "\n",
    "# testing quality of a linear model for predicting MEDV from LSTAT using k-fold crossvalidation\n",
    "\n",
    "print '------------- Estimating/Evaluating model by crossvalidation -------------'\n",
    "k = 5\n",
    "kfolds = model_selection.KFold(n_splits=k)\n",
    "\n",
    "mse,r_squared = np.zeros((k,)), np.zeros((k,))\n",
    "i = 0\n",
    "for (train,test) in kfolds.split(XX):\n",
    "    X_train, X_test, y_train, y_test = XX[train], XX[test], y[train], y[test]\n",
    "    lr = LinearReg_SquaredLoss()\n",
    "    J_history = lr.train(X_train,y_train,learning_rate=0.005,num_iters=10000,verbose=True)\n",
    "    mse[i],r_squared[i] = evaluate_model(lr,X_test,y_test)\n",
    "    i = i + 1\n",
    "\n",
    "print k, \" fold cross_validation MSE = \", np.mean(mse)\n",
    "print k, \" fold cross_validation r_squared = \", np.mean(r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
